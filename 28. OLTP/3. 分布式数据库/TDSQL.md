# 背景

## 腾讯云数据库

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAD79.tmp.jpg) 

关系型数据库：MySQL、MariaDB、SQL Server、PostgreSQL、CynosDB（TDSQL-C）

非关系型数据库：Redis、MongoDB、Memcache、TcaplusDB、CTSDB

分布式数据库：TDSQL（OLTP）、TBASE（OLAP）

数据库SaaS产品：DTS、DBBrain、云图

 

## 业务挑战

### **数据一致性**

对某些业务（如金融业务）来讲，数据的强一致（Consistency）尤为重要。 如果出现数据丢失，就意味会给组织或用户带来直接的金钱方面的损失，甚至影响企业的商誉和信誉。因此，数据的一致性是数据库管理员(DBA)最需要考虑的问题之一。

然而，多数开源不适用于共享存储架构，基于主从高可用架构难以做到既满足性能又保障主库出问题时数据不丢失，无法满足业务高并发需求。

 

### **服务可用性**

随着业务需求的不断提高，搭建一个数据库高可用环境已经成为很多企业迫切的需求。确保企业中计算资源的持续可用是各个数据库管理员(DBA)的主要目标。如果支持应用程序的数据库和服务器不可用，会导致大量投诉或用户流失， 甚至带来金钱方面的损失，影响信誉和商誉。高可用性和减少停机时间是数据库系统的目标，某些业务甚至需要24*7无障碍运行 。

### **扩展性**

业务在采购之初很难准确预测未来业务增长的速度和总量，这就导致业务不得不采购比自己实际需求更多的资源。这可能导致：资源的浪费，您可能利用了10%的资源，而浪费了90%；或者难以扩展：您的业务发展可能远超预期，您又不得马不停蹄地采购更高配置的资源，不断的停机迁移。当然，scale-out（横向扩展）的分布式架构可以解决了这个矛盾，但目前这一起步门槛会较高。

 

### **信息安全**

在这个大数据时代，数据和数据库安全比以往任何时候都更加珍贵。一旦数据发生泄露，那么付出的代价将是非常惨痛的。由于数据泄露而导致的业务中断、客户信心丧失、法律成本、监管罚款，这些后果可能导致数百万的花费甚至是灾难性的。

 

### **数据库优化**

随着业务的发展，数据库数量越来越多，如何保障所有数据库做到性能优异，业务不出问题；这对数据库管理员（DBA）提出诸多要求，在了解数据库基础运维知识的基础上，还要求DBA对SQL优化，性能检测，甚至业务逻辑和业务编程的了解。随着业务的快速发展，这种重人工模式意味着DBA不可能“照顾”到所有数据库的，那是否能将机器学习、深度学习这样的技术引入到数据库领域，帮助DBA更好的优化数据库呢？

 

## 传统数据库 vs 分布式数据库

传统数据库面临的挑战：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAD7A.tmp.jpg) 

### **分库分表**

***\*1、传统数据库+分库分表方案\****

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAD7B.tmp.jpg) 

Shared-everything只能垂直扩容，局限较大。

***\*分库分表方案面临的问题：\****

1）数据不断增加，出现性能瓶颈

2）分库分表方案，手动定制分库分表规则（其实，不能说手动定制就不好，这样反而灵活性更高）

3）扩容对业务影响较大，需要暂停业务停机扩容

4）OLAP与OLTP分开存储成本上升，数据复用低

5）业务改造成本高，数据迁移同步依靠经验评估存在误差

 

***\*2、分布式数据库Shared-nothing架构\****

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAD8B.tmp.jpg) 

1）自动分片存储：分布式数据库结构对业务透明，逻辑上是一个统一的整体，在物理上则是将数据通过路由以及分片算法方式，将数据打散分别存储在不同的数据节点上。

2）兼容传统DB：分布式数据库是在集中式数据库的基础上发展起来的，对传统数据库具有较好的兼容性，兼容SQL2003标准，无需额外的学习成本。

3）在线扩展性：分布式数据库可以通过线性的水平扩展各部分组件来增加数据库的性能，且在扩展时可以做到在线扩展，无需业务暂停或中断。

4）海量数据支持：分布式数据库支持海量数据的业务，且可以根据业务需求灵活的扩容扩展，且未来发展可以做到OLTP、OLAP业务实现混合支撑。

### **垂直/水平拆分**

***\*1、传统数据库方案：垂直拆分\****

垂直切分，即分库

垂直拆分，即拆分业务

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAD8C.tmp.jpg) 

按功能切分数据库，该方法与业务紧密相关，如电商平台，数据库实例，如果表设计的单纯按照会员关联所有业务，所有的数据全集中到一台服务器检索效率必然低下。按功能切分为会员库、商品库、交易库、物流库等多个实例，共同承担业务压力。

垂直拆分不能解决压力问题，因为单台服务器的负载和容量是有限的，随着业务的发展会成为瓶颈，解决这些问题的常用方案是水平拆分。

2、传统数据库：水平拆分

水平切分，即分表

水平拆分，分数据量

按照某个规则，将一个表的数据分散到多个物理独立的数据库服务器中；多个这样“独立”的数据库“分片”组成一个逻辑上完成的数据库实例。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAD8D.tmp.jpg) 

比如我们可以再次按照月份/年份来分不同的字段表存放数据。通常来说，分表的前提是分库。

### **对比**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAD9E.tmp.jpg) 

# 概述

腾讯分布式数据（Tencent Distributed SQL，TDSQL）是腾讯研发的一套基于MySQL协议的国产分布式数据库，同时兼容MySQL、mariadb两个内核分支。

它适用于超大并发、超高性能、超大容量的OLTP交易类业务场景的分布式数据库。

采用***\*主从高可用架构\****，提供弹性扩展、备份、恢复、监控等全套解决方案，有效解决业务快速发展时数据库面临的各种挑战。

 

***\*产品定位：\****金融级数据库保证的分布式数据库

***\*部署方式：\****TDSQL可以提供公有云、专有云两种部署方案，同时提供TDSQL一体机解决方案，可***\*提供关系型数据库实例、分布式数据库实例、分析性数据库\****实例。

***\*自身能力：\****TDSQL具备强同步复制、线程池、热点更新、内核优化等能力，能够为用户提供事前、事中、事后的多维度的安全方案，并获得了多项国际和国家认证。

 

## 发展历程

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAD9F.tmp.jpg) 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADA0.tmp.jpg) 

公有云有CDB（Correlate DataBase）和DCBD（Distributed Correlate DataBase）两个分支。

具体版本：

NoShard版本，不分片，完全兼容MySQL

Shard版本，分片，解决超大并发、超大数据量的场景，有一些语法的限制

Spark版本，主要面向OLAP。

注：这个类似GoldenDB的单机数据库和分布式数据库。

 

## 优点

1、产品自主可控

自主可控：TDSQL开发了具有腾讯自主知识产权的内核分支TXSQL

开源备用：持续参与mariadb，postgresql开源社区支持和赞助

国产适配：积极适配国产芯片、服务器、操作系统，如海思、华为、中标麒麟等。

2、数据安全保障

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADA1.tmp.jpg) 

3、便捷运维管理

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADB1.tmp.jpg) 

4、云上云下灵活部署

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADB2.tmp.jpg) 

## 部署

### **公有云部署**

### **私有云部署**

### **一体机**

### **虚拟机部署**

 

## 功能

***\*主要技术特点：\****

1、支持分库分表技术，单节点事务，不支持分布式事务

2、支持高性能的主备数据强同步方案，支持自动的跨IDC容灾切换，并能保证数据一致性，对于提交成功的事务确保不丢失

3、灵活的容量伸缩机制，对业务透明

注：上述技术特点可能已经过时，需要参考最新的官方文档。

 

***\*已经支持的特性：\****

1、提供了灵活的读写分离模式

2、支持全局order by、group by、limit

3、聚合函数支持sum、count、avg、min、max（其他聚合函数不支持）

4、基于分组以及广播表实现常用的跨SET的join

5、支持预处理协议

6、支持全局唯一字段

7、支持分布式事务

8、支持SSL加密传输

9、提供特定的SQL查询整个集群的配置和状态

暂时不支持的特性：

1、自定义函数

2、视图、存储过程、触发器、游标

3、外键、自建分区

4、复合语句，如BEGIN、LOOP、UNION等

5、子查询、having

只支持修改session级别的系统变量，如果需要设置global级别的，需要从专门接口操作。

支持MySQL所有数据类型。

支持大多数函数，主要限制是聚合函数。

 

## CAP理论

## 适用场景

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADB3.tmp.jpg) 

# 架构

## 整体架构

### **TDSQL**

TDSQL整体架构：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADC4.tmp.jpg) 

TDSQL架构简图：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADC5.tmp.jpg) 

注：数据采集及监控对应GoldenDB的管理节点（元数据、集群、proxy管理）。

TDSQL采用分布式集群架构，这种集群架构具有较高的灵活性， 简化了各个节点之间的通信机制，也简化了对于硬件的需求。 这不仅意味着TDSQL的关系型实例、分布式实例、分析性实例可以混合部署在同一集群中，也意味着即使是简单的x86服务器，也可以搭建出类似于小型机、共享存储等一样稳定可靠的数据库：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADC6.tmp.jpg) 

***\*实例：\****从业务视角看到的一个具有完整能力的数据库；
	***\*分片(Sharding)：\****是由数据库节点组（SET）和SQL Engine（SQL Engine）和支撑系统组成一主多从数据库，也是水平拆分后承载数据的基本单元；
	***\*节点组（SET）：\****由数据库节点（DataNode）组成的，通常包括一个主、从节点的集合。
	***\*说明：\****云数据库支持虚拟化多租户能力，节点即可以是物理节点（一台物理设备），也可以是逻辑节点（一台物理设备的一部分资源）。

***\*SQL引擎层（SQL Engine）：\****账号鉴权、管理连接、SQL解析、分配路由的SQL Engine模块；SQL Engine可以混合部署在数据库节点（DataNode）之上，也可以独立部署在一台物理机中。SQL Engine也是采用分布式架构设计，提供并行负载和高可用容灾能力；
	***\*调度集群、决策集群：\****作为集群的管理调度中心，主要保证数据库节点组、接入SQL Engine集群的正常运行；
	⚫ 调度集群（Scheduler）：帮助DBA或者数据库用户自动调度和运行各种类型的作业，比如***\*数据库备份、收集监控、生成各种报表或者执行业务流程\****等等，TDSQL把Schedule、Zookeeper、Oss（运营支撑系统）结合起来，通过时间窗口激活指定的资源计划，完成数据库在资源管理和作业调度上的各种复杂需求， Oracle也用DBMS_SCHEDULER支持类似的能力。
	⚫ 决策集群（ZooKeeper）：在TDSQL中，它的主要功能是配置维护、选举决策、路由同步等，ZooKeeper支撑数据库节点组（分片）的创建、删除、替换等工作，集群部署要求大于等于3组且跨机房部署。
	***\*TDSpark节点：\****基于Spark扩展的计算节点，采用只读的方式与SET连接，以JDBC的方式获取数据。
	***\*赤兔运营平台（chitu）：\****基于TDSQL定制开发的一套综合的业务运营和管理平台，将数据库的管理特点，将网络管理、系统管理、监控服务有机整合在一起。 

### **DCBD**

DCBD架构：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADC7.tmp.jpg) 

注：这里的Set是逻辑的概念，类似dbgroup，只不过备机需要分布在不同的dbgroup。

 

## SQL引擎

计算节点（SQL引擎）也被称为proxy或网关。在TDSQL中位于接入层的位置，属于CPU密集型服务。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADD8.tmp.jpg) 

注：Proxy不负责执行计划树的解析。

## SET数据节点

SET：TDSQL最小数据单元，包含一主n备n+1个数据节点。

1）数据节点上部署MySQL数据库服务

2）Agent属于旁路模块，主要承担MySQL实例的状态监控

3）SET的路由信息由Zookeeper记录

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADD9.tmp.jpg) 

## RocksDB引擎

RocksDB是由Facebook开源的一套基于LSM的KV存储系统，通常适用于写量超大业务场景，例如物联网，电商订单等场景。目前RocksDB已在TDSQL的分布式实例和关系型实例中支持，您可以在创建实例是选择RocksDB。使用RocksDB后，您也可以在建表时显示指定InnoDB或XtraDB引擎。当然，腾讯RocksDB支持MySQL协议的语法，支持数据库多版本并非控制(mvcc)，（分布式）事务(2PC)，（分布式）JOIN，主从数据复制，备份恢复等关系型数据库特性。而RocksDB强大的写性能优异性主要来自于LSM Tree算法。
	LSM Tree是一种***\*将随机写合并为顺序写的算法\****（如下图），数据以KV值有序存储，随机写入数据库时，先以树状结构（tree）组织更新在内存中；由于每一个数据的KV值是全局有序的， 随着写入的增加， 内存的tree不断变大（丰富其树状枝丫），当一定程度后，相同层级的tree值发出与磁盘的tree合并操作。一次性批量写入已经组织好的有序数据，即将随机写入，通过LSM组织为顺序写入。这样就大大的提高了写入效率和并发。 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADDA.tmp.jpg) 

 

# 操作

## 实例

TDSQL支持两种实例类型的创建：

1、非分布式实例（传统关系型实例）

非分布式实例为主从架构的关系型数据库，完全兼容MySQL。

2、分布式实例

分布式实例为主从架构的分布式数据库，高度兼容MySQL。

***\*操作步骤：\****

1、在赤兔管理平台节点，点击【实例管理】，进入实例管理界面

2、点击【创建分布式实例】，系统弹出【创建实例[分布式]】对话框

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADDB.tmp.jpg) 

3、子SET资源配置

我们可以根据资源情况分配内存，CPU，磁盘空间，日志空间等，也可以手动输入定制资源大小。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADEB.tmp.jpg) 

4、子SET容灾模式

设置子SET的容灾模式、主备DB部署区、同步异步模式、服务器IP等。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADEC.tmp.jpg) 

5、子SET初始化

设置子SET的字符编码、排序规则、大小写是否敏感等。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADED.tmp.jpg) 

***\*6、业务实例选型参考：\****

集中式 vs 分布式

Noshard：非分布式版本即传统集中式，使用起来完全和MySQL一样。

Shard：分布式版本，适用于单节点已经无法满足负载要求，且未来还可能有大量增长需要将数据分散到其他节点。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADEE.tmp.jpg) 

业务选型：中小规模适用于noshard，大规模适用于shard。

## 分布式表

在单实例（noshard）模式下，一张库表分布在一个MySQL实例上。

在分布式（shard）模式下，一张表根据分片的数量分布在不同的MySQL节点上。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsADEF.tmp.jpg) 

***\*如何使用shardkey：\****

1、创建表时需要指定路由字段shardkey

2、业务SQL的增、删、改、查包含shardkey时，SQLEngine通过对shardkey进行hash

3、数据根据分片算法，将SQL发往对应的分片，如果SQL中不带shardkey，则该SQL会发往所有的分片

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE01.tmp.jpg) 

***\*分布式表的数据拆分：\****

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE02.tmp.jpg) 

***\*按照shardkey拆分：\****

1、将数据打散的很自然的一个字段，如用户ID，微信ID等

2、不同的SET负责不同范围的key值，SQL引擎根据SQL中的shardkey值hash计算后发往对应的SET

 

***\*建表：三种模式\****

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE03.tmp.jpg) 

注：这里的分表、单表、广播表类似GoldenDB中的hash表、单节点、多节点复制表。

### **分表**

分表，即水平分表（又名shard表），分表默认需要指定一个字段为shardkey。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE04.tmp.jpg) 

Shardkey的选择：

1、主键以及所有唯一索引的一部分

2、建议选择业务特点和数据库中每张表里实体关系，找到一个与大多数表都相关的实体（字段）作为拆分键

 

### **单表**

单表（又名noshard表），主要用于存储一些无需分片的表：该表的数据全量存在第一个物理分片（set）中，所有该类型的表都放在第一个物理分片中，语法和使用规范和mysql完全一样，可以理解为一个非分布式的表。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE14.tmp.jpg) 

### **广播表**

广播表：即又名小表广播功能，设置为广播表后，该表的所有操作都将广播到所有物理分片（set）中，每个分片都有该表的全量数据，常用语业务系统的配置等。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE15.tmp.jpg) 

数据自动拆分、自动聚合：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE16.tmp.jpg) 

## DDL

与MySQL语法一直，也支持alter table，增删索引等。

但是不支持rename分表的表名，修改shardkey字段的名字；如有必要，徐采用先加后删的方式。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE17.tmp.jpg) 

### **Shardkey**

主键（唯一索引）vs shardkey

主键和所有唯一索引必须包含shardkey。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE27.tmp.jpg) 

### **二级分区**

支持range和list二级分区，支持数值，字符和时间类型，支持年月日三种函数。

支持增加，删除二级分区，语法和MySQL分区一样。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE28.tmp.jpg) 

二级分区类似GoldenDB中的分发键之后设置分区。

 

### **全局唯一字段**

全局唯一字段和MySQL自增字段语法一样，保证全局唯一，但是无法保证严格递增（因为每个proxy有本地缓存）。

支持last_insert_id。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE29.tmp.jpg) 

注：使用ZK设置全局自增字段，在使用的时候与单机一样，每个proxy会缓存一部分，所以这样无法保证严格自增。

## DML

Insert/replace字段必须包含sharkey（即使value中使用全部字段），佛足额会拒绝执行该SQL。

## DQL

select最好带上shardkey，否则就需要全表扫描，然后网管进行结果集聚合，影响执行效率。

JOIN：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE2A.tmp.jpg) 

注：直接下发到DB执行join操作，proxy仅聚合结果，不需要计算。

Noshard表之间：无限制

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE3B.tmp.jpg) 

广播表：与所有表都可以无限制JOIN

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE3C.tmp.jpg) 

广播表一般用于读较多，但是写操作比较少的情况（不同节点之间需要同步是会耗时的）。

 

## DCL

### **查询集群状态**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE3D.tmp.jpg) 

### **Explain**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE3E.tmp.jpg) 

### **权限限制**

1、不支持grant，需要使用接口进行授权，参考OSS文档

2、支持IP白名单

3、支持SQL防火墙，参考数据库防火墙

4、权限设置和MYSQL完全一样

通过IP透传，数据库得到的是真正的客户端IP

 

 

# 事务与并发

分布式事务采用两阶段提交协议实现。

在异常处理方面，全局死锁检测机制。

 

分布式事务，就是一个数据库事务在多个数据库实例上面执行，并且多个实例上面都执行了写入（insert/update/delete）操作。实现分布式事务处理的最大难点，就是在多个数据库实例上面实现统一的数据库事务的 ACID 保障，而这里面最重要的算法就是两阶段提交算法。分布式事务能力理论虽然很早就被提出，而业内实际工程化实现和大规模业务验证的产品还较少。 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE4E.tmp.jpg) 

TDSQL支持分布事务，可以为银行转账、电商交易等业务提供有效支持。当然，分布式事务处理的开销会比单机架构事务处理开销要大一些，使用分布式事务会导致系统TPS降低，事务提交延时增大。而腾讯TDSQL通过多种优化，提供了高于开源XA（分布式事务简称）的性能。 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE4F.tmp.jpg) 

由于理论上，一个事务不会操作全部分片，仅操作1~2个分片（如转账业务），再加上TDSQL的MPP架构的原因，因此一个分布式实例多个分片的分布式事务性能可以叠加。 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE50.tmp.jpg) 

所以是否使用分布式事务要根据实际应用需求来定。数据量非常大或者数据访问负载非常高时，分布式事务会大大降低应用开发难度，TDSQL每个事务的查询语句的写法与使用单机架构实例完全相同，且获得事务的ACID保障。对于涉及跨分片的分布式事务，我们建议业务开发时，平衡性能和开发难度的关系，或将事务拆解，巧妙设计或引入一些等待机制，以优化用户体验。

 

# 数据分布

## 自动拆分

关系型数据库是一个二维模型，数据的切分通常就需要找到一个分表字段（shardkey）以确定拆分维度，再通过定义规则来实现数据库的拆分。业内的几种常见的分表规则如下：
	⚫ 基于日期顺序（Time），如按年拆分，2015年一个分表，2016年一个分表。
	⚫ 基于某字段划分范围（Range），如按用户ID划分，0~1000一个分表，1001~2000一个分表。
	⚫ 基于某字段求模（Hash），将求模后字段的特定范围分散到不同库中。
	无论是Time、Range都容易导致严重的数据倾斜，即分片之间负载和数据容量严重不均衡。例如，在大部分数据库系统中，数据有明显的冷热特征——显然当前的订单被访问的概率比半年前的订单要高的多——而采用Time分表或range分表，就意味着很容易出现大部分热数据将会被路由在少数几分片中，而剩下的分片设备性能却被白白浪费掉了。因此，TDSQL通常采用某个字段求模（Hash）的方案进行分表。因为Hash算法的原理能够基本保证数据相对均匀的分散在不同的物理设备中。

注：GoldenDB也存在这种数据倾斜的问题，还是需要结合具体数据和业务去考量。

基于上述原理，TDSQL分布式实例在创建表的时候，要求SQL中显示指定拆分建shardkey，例如：create table tb1 ( user_id int not null,age int not null, place char(20) not null,primary key(user_id, age) ,unique key(user_id, place)) shardkey= user_id;此时，Hash的过程大致就是，当某条记录(如下图)请求时被发起时，TDSQL会理解SQL语句的含义，然后将拆分键shardkey（此处为user_id）的值进行Hash，根据Hash后的值和SQL Engine中预设的路由表进行匹配，然后将SQL路由到对应分片（或指定节点）执行。 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE51.tmp.jpg) 

如果一个查询SQL语句的数据涉及到多个分表，此时SQL会被路由到多个分表执行，TDSQL会将各个分表返回的数据按照原始SQL语义进行合并，并将最终结果返回给用户。 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE52.tmp.jpg) 

查询SQL在处理逻辑上，分为两类情况：如果SQL有明确shardkey值，数据将直接从对应的分片取出，此时效率最高；如果SQL没有shardkey，SQL请求将发往所有分片，并在SQL Engine中聚合在反馈给业务，此时效率会略差。 从上述原理来看，查询SQL中含有shardkey值比不含shardkey值效率将会更高。

 注：TDSQL这种自动拆分的方案虽然比较方便，但是不够灵活，相对GoldenDB的拆分方案就有些不够灵活了。

 

## 逻辑表

TDSQL对应用来说，读写数据完全透明，对业务呈现的表实际上是逻辑表。逻辑表屏蔽了物理层实际存储规则，业务无需关心数据层如何存储，只需要关注基于业务表应该如何设计。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE63.tmp.jpg) 

TDSQL为用户提供了三种类似的表：分表，广播表及单表：
	⚫ 分表：是指那些原有的很大数据的表，需要切分到多个数据库的表，这样每个分片都有一部分数据，所有分片构成了完整的数据。（仅分布式架构实例可使用）
	⚫ 广播表：名小表广播功能，设置为广播表后，该表的所有操作都将广播到所有物理分片（set）中，每个分片都有改表的全量数据。（仅分布式架构实例可使用）
	⚫ 单表：即无需拆分的表，又叫做普通表，目前单表都放在第一个物理分片（set）中。 

## 拆分键选择

拆分键是在水平拆分过程中用于生成拆分规则的数据表字段。TDSQL建议拆分键要尽可能找到数据表中的数据在业务逻辑上的主体，并确定大部分（或核心的）数据库操作都是围绕这个主体的数据进行，然后可使用该主体对应的字段作为拆分键，进行分表（该分表方案叫做groupshard），如下图： 

 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE64.tmp.jpg) 

Groupshard的分表方案，可以确保某些复杂的业务逻辑运算，聚合到一个物理分片内。例如，某电商平台订单表和用户表都是基于用户维度（UserID）拆分，平台就可以很容易的通过联合查询（不会存在跨节点JOIN，或分布式事务）快速计算某个用户近期产生了多少订单。 

下面的一些典型应用场景都有明确的业务逻辑主体，可用于拆分键：
	⚫ 面向用户的互联网应用，都是围绕用户维度来做各种操作，那么业务逻辑主体就是用户，可使用用户对应的字段作为拆分键；
	⚫ 电商应用或O2O应用，都是围绕卖家/买家维度来进行各种操作，那么业务逻辑主体就是卖家/买家，可使用卖家/买家对应的字段作为拆分键；但请注意，某些情况下几个超大卖家占到绝大多数交易额，这种情况会导致某几个分片的负载和压力明显高于其他分片；

⚫ 游戏类的应用，是围绕玩家维度来做各种操作，那么业务逻辑主体就是玩家，可使用玩家对应的字段作为拆分键；
	⚫ 物联网方面的应用，则是基于物联信息进行操作，那么业务逻辑主体就是传感器/SIM卡，可使用传感器、独立设备、SIM卡的IMEI作为对应的字段作为拆分键；
	⚫ 税务/工商类的应用，主要是基于纳税人/法人的信息来开展前台业务， 那么业务逻辑主体就是纳税人/法人，可使用纳税人/法人对应的字段作为拆分键；
	以此类推，其它类型的应用场景，大多也能找到合适的业务逻辑主体作为拆分键的选择。 

## 拆分键限制

为了提高语法解析效率，避免因为shardkey设置导致路由错误，TDSQL规定了拆分键设定的技术限制（更多详情，请参考腾讯云官方文档）：
	⚫ Shardkey需要是主键以及所有唯一索引的一部分；
	⚫ shardkey字段的类型必须是int,bigint,smallint/char/varchar
	⚫ shardkey字段的值不应该有中文，SQL Engine不会转换字符集，所以不同字符集可能会路由到不同的分区
	⚫ 不要update shardkey字段的值
	⚫ shardkey=a放在sql的最后面
	⚫ 访问数据尽量都能带上shardkey字段，这个不是强制要求，但是不带 shardkey的sql会路由到所有节点，消耗较多资源 

# 复制/一致性

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE65.tmp.jpg) 

## MySQL原生复制

异步：主机不等备机应答直接返回客户端成功

半同步：主机在一定条件下等待备机应答再返回客户端成功

## TDSQL复制

强同步：主机等待至少一台备机应答成功后才返回客户端成功。是TDSQL数据主备一致、不出现丢失最核心的保障。

强同步机制：任何一笔应答前端成功的请求除了在主机落盘成功外还会在至少一台备机落盘成功。

强同步性能：在强同步复制的基础上做了性能优化改进，性能几乎和异步复制达到一样的效果。

 

由于数据库中记录了数据，想要通过高可用架构实现切换，数据必须是完全一致且同步的，所以数据同步技术是数据库高可用方案的基础，通常数据同步的流程如下图：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE66.tmp.jpg) 

当前，开源MySQL数据库数据复制包括异步复制、半同步复制两种。这两种复制技术的主要问题是，节点故障时，有可能导致数据丢失或错乱。而且，这类复制技术以串行复制为主，性能相对比较低。而腾讯自主研发了的基于MySQL协议的并行多线程强同步复制方案（Multi-thread Asynchronous Replication， MAR），在应用发起请求时，只有当从节点(Slave)节点返回成功信息后，主节点（Master）节点才向应用应答请求成功（如下流程图）；这样就可以确保主从节点数据完全一致。 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE77.tmp.jpg) 

说明：使用“强同步”复制时，如果主库与备库自建网络中断或备库出现问题，主库也会被锁住（hang），而此时如果只有一个主库或一个备库，那么是无法做高可用方案的。（因为单一服务器服务，如果故障则直接导致部分数据完全丢失，不符合金融级数据安全要求。）因此，TDSQL在强同步技术的基础上，提供强同步可退化的方案，方案原理类似于半同步，但实现方案与google的半同步技术不同。

另外，TDSQL强同步将串行同步线程并行化，引入工作线程能力，大幅度提高性能；对比在跨可用区(IDC机房，延迟约10~20ms)同样的测试方案下，我们发现MAR技术性能优于MySQL 5.6的半同步约5倍，优于MariaDB Galera Cluster性能1.5倍，在OLTP RW(读写混合，主从架构)，是MySQL 5.7异步的1.2倍（如下由英特尔®技术团队测试的性能图）：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE78.tmp.jpg) 

为进一步验证强同步数据一致性，我们在每秒插入2万行数据的场景下，直接杀掉主机数据库进程，并在切换备机后导出流水做对比，发现数据完全一致。

 

# 备份恢复

## 备份

TDSQL备份都是在赤兔平台完成的，包括两种：手动备份，自动备份。

### **备份存储类型**

备份存储类型有：

1、HDFS：备份到分布式文件存储系统中，需要预先配置HDFS存储

2、Local：指备份到DB本地磁盘中

3、COS：指备份到腾讯云分布式存储中

### **备份类型**

备份类型有：

1、物理备份：指物理数据的全备份

2、逻辑备份：产生对应的SQL语句再执行的过程，逻辑备份是物理备份的补充。

#### **物理备份**

实例申请后将自动开启物理备份，每天业务低峰（凌晨00:00~06:00）将自动全量备份数据。

1、默认会开启全量物理备份，关闭全量逻辑备份

2、全量物理备份支持增量备份，可以设置增量备份间隔天数

3、备份周期：默认每天做一个全量备份

4、备份保存时间：默认保存30天，可以在运营平台修改保存时间

#### **逻辑备份**

### **工具**

TDSQL兼容的备份和数据迁移工具：

1、mydumper和myloader备份或数据迁移

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE79.tmp.jpg) 

2、Mysqldumper备份或数据迁移

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE7A.tmp.jpg) 

注：mysqldumper是单线程的，性能较差，使用的时候一定要加--single-transaction，如果不加会锁表。

3、Load data备份或数据迁移

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE8A.tmp.jpg) 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE8B.tmp.jpg) 

## 恢复

恢复又称为回档，包括逻辑回档和物理回档。

回档时间为实际备份保留的时间，如果备份损坏或被删除，可能导致回档失败。

若无特殊要求，回档创建的新临时实例建议与原实例保持相同规格。

### **闪回机制**

# 兼容性

## 全局唯一序列

数据切分后，原有的关系数据库中的主键约束在分布式条件下将无法使用，因此需要引入外部机制保证数据唯一性标识，这就是全局唯一数字序列（sequence）。
	TDSQL全局唯一数字序列（以下简称sequence，使用的是unsigned long类型，8个字节长），使用方法与MySQL的AUTO_INCREMENT类似。目前TDSQL可以保证该字段全局唯一和有序递增，但不保证连续性。 

 

## 分布式JOIN

业务逻辑中，经常需要查询两个或多个表中的列之间的关系（JOIN），这在关系型实例（单机架构）上的简单操作， 在分布式实例中就比较复杂。 由于需要JOIN的数据可能分别存储在多个物理节点中， 导致JOIN过程需要大量网络交互，这导致某些分布式数据库处理JOIN请求时，无法提供数据一致性和性能兼得的方案。TDSQL业内少数几个支持分布式JOIN，且在大规模业务中验证过的产品。一般来说，分布式JOIN分为可下推和不可下推：

可下推JOIN，是指可在存储层直接JOIN的情况，通常包括：
	⚫ 同纬度（拆分建）的JOIN：两张表采用相同的拆分键，例如：SELECT * FROM user JOIN user_order ON user.user_id=user_order.user_id;由于user与 user_order均已user_id为拆分键，因此同一用户（user_id）的记录位于同一分片上，JOIN直接由底层出错了完成。此时性能最高。
	⚫ 分表与广播表的JOIN：由于所有分片中都存在一个完整的广播表副本，因此分表与广播表的JOIN也可下推到存储层执行。

 

不可下推的JOIN，是指需要由存储层和SQL Engine共同完成的JOIN，通常包括：单表与分表的JOIN，分表与分表且不同字段的JOIN等。腾讯云优化不可下推的分布式JOIN，并采用如下的过程执行（如下图）。 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE8C.tmp.jpg) 

另外，分布式实例也支持子查询、函数等复杂语句。 

 

## 兼容JSON

TDSQL的分布式实例、关系型实例已在（MySQL 5.7内核）已支持JSON功能，对比于MongoDB目前的三大核心功能：JSON的灵活性，复制集群保证高可用，sharding保证可扩展，TDSQL均可以支持，基于腾讯云金融级特性，无论是其自身数据强一致、高可用和可扩展也有着完善的解决方案，且能够支持关系型数据库的事务，JOIN等功能。如果您既希望使用JSON类型，又对数据一致性，事务，JOIN 等传统数据库具备的能力也有一定要求的话，TDSQL将是一个很好的选择。
	而通过对比TDSQL与MongoDB在JSON方面支持的区别，我们注意到：
	⚫ JOIN： TDSQL支持多表根据JSON字段进行JOIN操作，MongoDB只支持多个unsharded表LEFT JOIN。
	⚫ Index：两者都支持根据JSON的某些（int,string）字段建立索引，MongoDB还额外支持multikey index等索引。
	⚫ 访问JSON内部元素： 两者都有各自完善的语法可以访问到JSON内部的各个字段，无需应用层进行JSON解析。
	⚫ 搜索条件：MongoDB提供的搜索和匹配方面的功能更完善，相比之下，TDSQL需要时刻注意对选择条件进行类型转换后再进行判断，对开发人员来说不是很友好，并且筛选的功能方面也较MongoDB 稍弱，适用于对JSON操作相对简单的应用。
	⚫ 写入数据：两者都可以以方便的写入JSON串和更新JSON内部的某些字段，但MongoDB不支持事务，只有单行操作可保证原子性，多行操作如果需要原子性需要应用层实现两阶段提交。
	而TDSQL的JSON操作可以完整的支持事务特性，也支持分布式事务。 

 

# 扩展性

TDSQL基于分布式架构和多租户方案，天生具有良好的弹性。这意味着数据库实例的并发性能、处理能力、存储容量可线性增长。 

## 弹性扩展

### **集群弹性扩展**

TDSQL是由一系列数据库节点组组成。集群的整体承载规模取决于集群中所有设备的总规模。若集群性能不足以支撑，可以通过更换更高配置的硬件、或
增加新的节点予以扩展。为便于DBA操作，TDSQL的赤兔运营平台提供自动化的实例迁移方案，DBA只需在设备初始化后，在系统中点击即可完成资源上线、集群扩容、实例迁移等操作。 

### **实例弹性扩展**

如果是关系型实例，除最大规格实例外均提供无缝升级功能。当遇到性能瓶颈时，只需在页面上通过鼠标点击操作，一键升级到更高性能和容量的实例规格，升级过程不影响业务正常访问和使用，并可指定在低谷期切换，以实现快速、平滑扩容，满足业务快速发展需要。

如果是分布式实例，由于其采用分布式架构，水平拆分逻辑，性能和容量均可以随分片的数量增长而线性增长（如下图）。由于分片数据可能存在不均衡情况，TDSQL提供新增分片，或扩容单个分片等的扩展方案，相关扩展方案仍然只需要在控制台上简单操作，即实现快速、平滑扩容。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE8D.tmp.jpg) 

TDSQL的分布式实例扩容，主要是采用腾讯自研的自动再均衡技术（Rebalance）保证自动化的扩容和稳定，以新增分片为例，扩容过程如下下图： 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE9E.tmp.jpg) 

\1) 控制台点击扩容A分片后，系统计算需要搬迁的数据，并开始配置G节点；
	2) G节点配置完成后，将A节点需要迁移的数据（通过从机）同步到G节点；
	3) 数据完全同步后，AG开始互相校验数据（存在1~几十秒的只读），但整个实例不会停止运行；
	4) 调度通知SQL Engine切换路由，完成后将A/G节点置位正常状态，A进入慢速删除状态。
	为确保业务不停以及数据一致性，TDSQL的整个迁移过程采用迁移存量数据、迁移增量数据、数据检验、再追增量、切换路由、清理六个步骤循环迭代进行。该能力经过腾讯内外海量业务迁移的检验，至今未发生过一次数据异常错误或全集群停机。

 

## 闲时超用技术

虚拟化让多个租户的业务共享物理设备性能，而传统隔离方案严格限制了每个租户实例的性能大小。这种限制方案很公平，但没有考虑到业务特点：大多数业务仅在一天（一月）的少数时刻有较大的业务压力（如下图）： 该业务日CPU平均使用率仅30%，而一天中仅存在7次业务压力较大，CPU使用率在80%~100%。虽然云能够基于弹性扩容，然而普通的弹性方案在这种突发性的压力面前，仍然无能为力——可能当您反应过来，您的业务峰值已过；最终，您还得基于业务峰值配置实例，浪费实例性能。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAE9F.tmp.jpg) 

***\*闲时超用技术，即在绝对保证每个实例预分配性能下限的基础上，允许实例使用超过预分配的性能\****。举个例子：假定A实例承载新闻业务，B实例是承载游戏业务， A、B实例被分配到一台物理设备中，A可以在B的空闲时间，抢占（有限的，并发全部）一部分空闲性能。当然，A、B同时面对峰值时，系统会确保A、B两实例底线的性能需求。
	相对于传统的方案，闲时超用是一种更加灵活的性能隔离方案，让您的业务在面对偶然性峰值时也能游刃有余。也经常用于实例之间性能的削峰填谷，节省成本。当然，在集群中实例相对较多分配较均衡的情况下，或已经预知实例之间可削峰填谷的情况下，闲时超用有较大意义。
	但若您的多个实例峰值点接近，开启闲时超用就不合适了。此时可以在赤兔运营平台中，关闭该功能。 

## 读写分离

TDSQL默认支持读写分离能力，架构中的每个从机都能支持只读能力，如果配置有多个从机，将由SQL Engine集群（SQL Engine）自动分配到低负载从机上，以支撑大型应用程序的读取流量。我们提供多种读写分离方案供您选择，且您无需关注若干从机是否完全存活，因为系统将根据策略自动调度。

⚫ ***\*只读帐号（推荐方案）\****：您仅需要在创建帐号时，标记为只读帐号，系统将根据只读策略向将读请求发往从机；只读策略可以根据主从延迟等维度进行灵活配置。
	⚫ ***\*/\*slave\*/注释（推荐方案）\****：您可以在编程过程中，通过注释/*slave*/，系统将把该条语句发往从机，常用于编程阶段将特殊的读逻辑嵌入代码。
	⚫ ***\*全局自动读写分离：\****您可以开启全局自动读写分离，该配置会自动将SQL中的读请求发向从机，且能识别事务、存储过程中的读语法并灵活处理。当然如果从机延迟较大，全局自动读写分离并不具备策略。
	⚫ ***\*只读实例：\****您也可以自建或申请只读实例，只读实例是专用于读请求的一种实例，不参与高可用切换。
	读写分离由此为您的应用提高总的读取吞吐量。通过多种只读方案的组合，您可以配置出复杂的只读方案，以满足您各种业务需求和开发的灵活性。

 

## 热点更新

在“秒杀”和“限时抢购”等这样的场景下，大量的用户在极短的时间内请求大量商品。而体现在MySQL数据库中，同一商品在数据库里肯定是一行存储，所以会有大量的线程来竞争InnoDB行锁，当并发度越高时等待的线程也会越多，TPS会下降RT会上升，数据库的吞吐量会严重受到影响。这会导致什么问题呢？
	⚫ 单个热点商品会影响整个数据库的性能，即 1 个商品做秒杀，影响整个平台性能和稳定性。
	⚫ 数据不一致，如 100 个库存结果卖出去 101 个。
	业内的通常采用引入多层架构，如热点缓存cache，热点库等方案。当然，这一类方案维护成本略高，且如果cache被击穿，则容易带来雪崩问题。
	而TDSQL的目标是让业务用尽量少的修改量（增加几个关键字的使用），便可以快速支撑热点更新功能，以为类似于秒杀，限时抢购等业务场景服务。同时对于已用缓存、热点库的场景下，在为业务进一步提高性能，减少故障发生概率，减少cache击穿带来的雪崩风险，提高平台整体稳定性。我们的解决方法如下：
	在SQL层，通过一个全局Hash表存储有INSERT/UPDATE请求的热点对象，其大小与热点对象上限相等。INSERT/UPDATE请求到达时，先查找Hash表中有无对应的热点对象，有就获取lock，会被阻塞；没有该热点对象，那么创建该热点对象，如果热点对象达到上限，那么返回错误，如果创建成功，那么持有lock并添加到Hash表。成功获取到热点对象lock，可以继续执行INSERT/UPDATE，否则等待lock被释放和系统调度到该线程。INSERT/UPDATE 返回后，释放热点对象lock，使得后续线程可以执行，如果后续线程不再INSERT/UPDATE 改热点对象，将其从Hash表移除（如下图）。 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEA0.tmp.jpg) 

## 异构数据库

# 高并发

## 闲时超用技术

## 热点更新

## 读写分离

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEA1.tmp.jpg) 

## 负载均衡

# 高可用

## 方案

## 容错/故障切换

在生产系统中，通常都需要用高可用方案来保证系统不间断运行；数据库作为系统数据存储和服务的核心能力，其可用要求高于计算服务资源。目前，TDSQL高可用方案通常是让多个数据库服务协同工作，当一台数据库故障，余下的立即顶替上去工作，这样就可以做到不中断服务或只中断很短时间，该方案简称主从高可用，也可以叫做主备高可用。在普通的主从高可用基础上， TDSQL支持：
	⚫ 支持故障自动转移，集群自动成员控制，故障节点自动从集群中移除；如果是实例级的主从切换，换后VIP（虚拟 IP）不变；基于强同步复制策略下，主从切换将保证主从数据完全一致，可满足金融级数据一致性要求。
	⚫ 支持故障自动恢复，承载分片的物理节点故障，调度系统自动尝试恢复节点，如果原节点无法恢复，将在30分钟内自动申请新资源，并通过备份重建（ Rebuild）节点，并将节点自动加入集群，已确保实例长期来保持完整的高可用架构。
	⚫ 每个节点都包含完整的数据副本，可以根据DBA需求切换；
	⚫ 支持免切设置，即可以设置在某一特殊时期，不处理故障转移。
	⚫ 仅需x86设备，且无需共享存储设备即可支持；
	⚫ 支持跨可用区部署，实例的主机和从机可分处于不同机房（无论是否同城），数据之间通过专线网络进行实时的数据复制。本地为主机，远程为从机，首先访问本地的节点，若本地实例发生故障或访问不可达，则访问远程从机。 若配合腾讯VPC网络环境下，可支持同城双活架构，即业务系统可以直接在两个中心读写数据库。跨可用区部署特性为TDSQL提供了多可用区容灾的能力， 避免了单IDC部署的运营风险。 

TDSQL的每一个分片都支持基于强同步的高可用方案，主数据库故障时将自动选举出最优备机立即顶替工作，切换过程对用户透明，且不改变访问IP。并且对数据库和底层物理设备提供7X24小时持续监控。发生故障时，TDSQL将自动重启数据库及相关进程；如果节点崩溃无法恢复，将通过备份文件自动重建节点（如下图）：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEB2.tmp.jpg) 

原理图：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEB3.tmp.jpg) 

特点：

1、主节点可读可写，备节点只读，任何时候只有一个主节点提供写服务，避免数据冲突

2、宁愿拒绝服务，不提供错误节点的写操作

3、当故障发生时，整个切换过程完全自动化，内部schedule自动进行节点选举升主，无须认为干预

4、严格的切换机制，确保主从节点切换前后数据的完全一致性

 

## 部署

### **同城双中心**

### **两地三中心**

# 数据安全

## 多项国家或国际认证

TDSQL现已代表腾讯云云数据库通过多项国家或国际认证，包括但不限于：ISO22301认证、ISO27001认证、ISO20000认证、ISO9001认证、可信云服务认证、信息安全等级保护（三级或以上）、CSA STAR认证、PCI DSS 1级服务提供商、SOC审计、ITSS云服务增强级认证等。

## 数据安全加密

TDSQL支持表空间加密（透明加密）和连接加密（SSL连接加密）。对于没有KMS（腾讯密钥管理服务）的场景，TDSQL支持密钥环服务，使内部服务器组件和插件能够安全地存储敏感信息以备以后检索，该服务包含了一系列API供加密功能调用密钥服务。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEB4.tmp.jpg) 

因为会有性能的损耗，默认是关闭的。

## SQL防火墙

SQL防火墙是对用户发送的SQL进行语法解析，过滤非法的SQL的一种安全能力。其与SQL Engine配合（如下架构图），可以对用户预先定义的一些非法SQL进行判断，从而对非法SQL进行过滤、阻断，有效的预防SQL注入或一些恶意非法攻击。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEB5.tmp.jpg) 

说明：SQL防火墙可以配合WAF等一起使用。考虑到业务情况和SQL复杂性，目前TDSQL的SQL防火墙暂未预存规则。

## 全维度的安全审计

安全审计是最重要的一种事后追溯手段，例如国家等级保护（三级）明确要求（7.1.3.3）明确要求信息系统支持审计能力。而全维度安全保障的云数据库系统来说，TDSQL包括三个个层面的审计能力（如下图），为用户提供完善的安全保护。

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEB6.tmp.jpg) 

其中，系统操作日志是赤兔运营平台自带的安全能力。数据库SQL审计是由腾讯云自研的数据库审计系统完成，公有云默认配置，专有云为选配。服务器操作审计是腾讯云自研的铁将军系统提供，公有云默认配置，专有云为选配。
	说明：在专有云中SQL审计、服务器审计是选件选配。 

## 内核级安全策略

TDSQL也在数据库内核层面提供了多种安全方案并开源，部分功能也已获得社区认可，在新版本中使用腾讯云提供的方案。此处以列举的方式，列举TDSQL的一些内核安全手段，例如：
	⚫ ***\*慢速删除\****：当用户执行drop table或者alter table ... drop partition时，数据库不是立刻删除表空间文件，而是将这些文件重命名并且在后台逐步缩小这些文件并最终删除。慢速删除可避免一次性删除巨大的表空间文件给服务器的文件系统带来突增的IO负载，导致系统出现波动。
	⚫ ***\*防止误删元数据\****：只允许通过规定登录方案的授权用户删除存储元数据的库表，以便防止用户用户误操作导致业务不可用。

⚫ ***\*禁止非授权用户安装插件\****：虽然数据库提供了标准的接口允许用户实现自定义的功能，但黑客经常利用这个漏洞以实现共计。因此，只允许规定的管理员用户挂载插件。
	⚫ ***\*禁止非授权用户访问物理服务器文件系统\****：在脆弱性报告中，黑客经常通过select into out file、注入文件、路径探测等的方式绕开安全系统，因此我们禁止非授权用户访问物理服务器的目录结构和文件系统。

注：这个可以在GoldenDB的loadserver用户下借鉴。

# 数据压缩

# 数据迁移

## DTS 

TDSQL支持选配数据传输工具（DTS），支持Oracle，Kafka等与TDSQL的互相同步；支持TDSQL实例之间的互相同步。同步拓扑支持一对一，一对多，多对一等方式，且支持冥等策略，以确保数据同步一致性。

 

## DB-bridge

 

# 运维/监控告警

# 分析型实例TDSpark

## 概述

TDSpark是TDSQL推出的为了解决用户复杂OLAP需求的解决方案。借助Spark平台本身的优势，同时融合TDSQL分布式集群的优势，为用户一站式解决HTAP（Hybrid Transactional/Analytical Processing）需求。
	众所周知，MySQL无法有效应对高计算强度的OLAP业务需求，通常需要借助ETL工具将数据同步到OLAP类数据库中；但该方案将极大的增加系统架构的复杂性。而TDSpark深度整合了Spark Catalyst引擎，并集成腾讯自研的且 MySQL协议的SQL Engine，利用TDSQL关系型实例或分布式实例现有数据，（通过从机）直接拉取到Spark引擎中进行计算。从数据集群的角度看， TDSpark可以让您直接在同一个平台进行事务和分析两种工作，简化了系统架构和运维。 

注：GoldenDB也采用MPP架构presto，但是对于具体哪些复杂语句需要拆分，以及直接对接OLAP还不够完善。

 

## 架构

TDSpark包括如下核心模块：
	OLAP SQL Engine：专门用于接收并处理客户端发来的OLAP类型的SQL。SQL Engine不与后端数据库相连，而是与运行在Spark集群集群上的Handle-sever交互。在获取到客户端发来的请求后，将SQL语句以JSON的形式发送至Handle-sever进行处理，并等待Handle-sever的响应。
	Handle-server：负责接收OLAP SQL Engine发来的JSON协议的请求，并给出响应。在接收到OLAP SQL Engine的请求后，对JSON串进行解析， 得到SQL 语句，同时对SQL进行语法解析得到库表名，并封装成独立的Spark job提交到Spark集群进行计算，待Spark得到结果后，将结果集转换成mysql协议并返回给SQL Engine。Handle-server部署在Spark集群的每一台机器上， SQL Engine可以访问Spark集群机器中的任意一台，并且该server可以随着Spark集群的扩容， 动态的增加服务节点。
	Spark集群在执行job的过程中，需要连接后端OLTP类实例（关系型或分布式实例）的SQL Engine，以JDBC的方式获取数据，在连接后端OLTP类实例的SQL Engine时，采用只读账户，从从机获取数据，从而降低对主机性能的影响。 

而Spark集群的资源管理由独立的Spark-manager进程完成。Spark-manager负责Spark集群工作节点的购买，资源回收，集群扩容等工作。多租户下，同一组机器资源Spark的部署方式如下图所示，每个模块之间资源隔离。 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEC6.tmp.jpg) 

Spark集群采用standalone方式部署，便于集群的扩容。当需要对集群扩容时，只需要增加一个worker节点并指向master，同时将Handle-server以client方式提交至集群即可。当集群需要缩容时，直接停止需要裁撤掉的worker即可。 

# 物理独享解决方案

# 专有云方案

## 部署架构

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEC7.tmp.jpg) 

TDSQL最小测试环境部署规划：

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEC8.tmp.jpg) 

 

## 模块选择

TDSQL是分布式架构的数据库，其部署架构和软件模块如下简图，其中浅黄底必须安装，浅蓝色为为可选配模块。 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEC9.tmp.jpg) 

### **核心模块**

***\*SQL Engine/DB：\****可以混合部署在同一物理机中，也可分离部署；此模块对CPU和磁盘性能要求较高，建议采用较高配置的CPU和SSD存储，并考虑高可用2台起部署。
	***\*赤兔运营系统：\****可部署在虚拟机或物理机中，建议为1/3/5等基数台部署。

### **选配模块**

***\*负载均衡模块：\****提供虚拟 IP，数据库负载；目前支持LVS、腾讯云网关TGW、 腾讯私有网络VPC等开源或商用负载；
	说明：如果不安装负载均衡模块，业务可以通过访问SQL Engine的IP和端口访问数据库，并通过连接池管理与多个SQL Engine的连接。
	***\*云数据库管理系统：\****云数据库管理系统需与腾讯专有云TCE合并安装。用于提供类似于集团云、行业云、政务云等场景下的租户端使用。
	说明：云数据库管理系统的部署依赖于赤兔运营系统。
	***\*数据备份系统：\****目前可以支持分布式文件存储系统HDFS，腾讯云对象存储 COS，网络存储NAS等。
	说明：如果不安装数据备份模块，将影响数据库备份、恢复与回档、备份与日志下载等功能。
	***\*OLAP扩展：\****指分析型数据库扩展，通常需要3台物理机起。
	说明：OLAP的部署依赖于SQL Engine/DB模块的部署。
	***\*支撑组件：\****指数据库审计、数据同步等其他功能，若不安装不影响数据库核心功能。具体需求请咨询腾讯相关工作人员。

 

## 建议设备

专有云资源池在设计时也应考虑到业务连续性目标和业务发展规划，根据业务性能需求、监管要求、业务性质和服务范围、数据集中程度、业务时间的敏感性、功能的关联性等要素进行业务需求分析，在此基础上评估业务中断可能造成的影响，确定灾难恢复需求，再通过需求决定资源池建设计划。一般来说，我们提供如下参考： 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEDA.tmp.jpg) 

测试环境配置： 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEDB.tmp.jpg) 

正式环境配置（中等档次）： 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEDC.tmp.jpg) 

正式环境配置（中高档次）： 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEDD.tmp.jpg) 

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEED.tmp.jpg) 

## 单中心容灾部署建议

单中心容灾时，数据库集群主要需要预防如下故障：
	⚫ 机房内交换机、负载转发或网卡等单点故障
	⚫ 机架电源、风扇、冷却等相关的单点故障
	⚫ 数据库服务器硬件的单点故障
	因此单中心容灾部署建议至少采用以下要求：
	⚫ 交换机、负载转发等网络设备至少是双活容灾
	⚫ 数据库服务器、管理调度建议采用一主二从模式部署
	⚫ 同一模块不同的设备需跨机架部署
	⚫ 需部署数据备份模块 

## 多中心容灾部署方案

### **同城双中心部署建议**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEEE.tmp.jpg) 

同城两中心时，建议至少采用三个节点的模式（也可以扩展为每个中心两节点（2+2 模式）），分布在两个机房，***\*机房之间采用专线互通，每组设备主机房部署2台，从机房部署 1台\****。负载网络采用类似于LVS的(DR模式)软负载均衡方案，建议能支持机房级故障网络也可以切换的。由于SQL Engine会自动分配请求，因此业务系统无论从哪个机房的访问，都可以访问到正常的DB。任何一个数据库节点或者机房当掉，TDSQL一般在40秒（含30秒检测时间）左右完成自动地切换，事务做到0丢失，业务确保重连机制即可。
	说明：腾讯云金融云（公有云金融专区），默认支持通常双中心架构。 

 

### **两地三中心部署建议**

![img](file:///C:\Users\大力\AppData\Local\Temp\ksohtml\wpsAEEF.tmp.jpg) 

两地三中心即在同城双中心的基础上，增加一个灾备中心。两个灾备实例之间，通过DCN方式进行同步，确保数据一致。 

注：两地的含义就是一个作为主数据中心，一个作为灾备。

## 腾讯专有云平台(TCE)

## 腾讯企业云平台(TStack)

# 应用场景